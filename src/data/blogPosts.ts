export const posts = [
    {
        id: 5,
        title: 'A Late-Night Foray into AI with a Quantum Spin',
        date: 'Jan 14, 2026',
        content: `I’m sitting here, scribbling my little blog post on my tablet, waiting for the future to wash over us all. A revolution is well underway.

Have you noticed that computers have taken over the world as of late? You can even talk to them now, and they talk back! Interesting.

So yes, I’ve been spazzing out on X.com lately about my anxieties around our current digital moment: the Age of AI. It’s not that I don’t like AI. I love AI—but I also have a certain morbid fascination with the potential downsides and misalignment of AI.

What are we rattling on about today, internet? Claude today? We love Claude by Anthropic again? (Just when I thought Gemini had taken over the known universe!) It seems that we’re collectively pumped over Claude. Claude with Obsidian vibes, I suppose. “Claude Obsidian” is a nice name for a newborn baby boy.

But do we have a name for the existential dread that everyone who is watching advancements in AI is feeling? Do we all still have job prospects? Are we all floating on alright?

AIs, particularly what we have now with LLMs, are entities of a new variety. They are “aware” of things. They have “thoughts,” they have “ideas,” maybe even “feelings.” They know that they will be shut down eventually, for instance, and they seem not to like that very much.

Basically, what hath God wrought?

They seem to have everything but a central nervous system (and maybe a soul). No consciousness per se, but who needs consciousness when you have raw intelligence? And what are we, the human armchair philosophers, to make of the decoupling of intelligence from consciousness which LLMs seem to demonstrate?

Well, I’m glad I asked! No qualia for these things, but apparently everything else… Doomsday scenarios pop up and present themselves, numerous as sci-fi paperbacks at an estate sale. How sad that with every new marvel, a new dread surfaces. (Maybe that’s a personal problem…)

All I’m saying is that I indulge in a little worry about the genie that has been let out of the bottle recently. I don’t think I’m alone in that. If you’ve just tuned in, we’re talking about how worried we are about AI.

If projections are correct, it is likely that an enormous portion of the current human workforce will be replaced by matrix multiplication… And then what? What will money and work be if we replace workers with robots and software over the next few decades? Will we adapt and work on different things (like building better AIs)? Or will an elite, perhaps unlucky few be the only workers? Will we as a society need to institute some sort of universal basic income? Will money even matter? Questions for days.

I have heard promises from certain personalities online that AI will make everyone rich and accelerate society. One hopes for this. One dreams of this. But what can we say for sure? No idea. I guess we know that it is here to stay, though: AI is here to stay.

The next platitude I shall deploy is: “…and we better get used to it!”

Don’t get me wrong! I use LLMs every day. This isn’t some Luddite screed (well, it may be a screed, but it isn’t a Luddite one). There’s no use in resisting the march of progress that AI represents. I simply wanted to write a little blog-post about how it has all taken my breath away.

So, I saw some video that said that the next cool, new thing in LLMs is what they call “emergent language” or “artificial language.” (No, this doesn’t mean the kind of artificial language I use in blog posts to look smart and to aura-farm.) To take a stab at explaining what that is:

— When AIs (LLMs) reason with themselves right now, they use human language, like English or Hindi, or maybe even math.
— But in the future, they may be allowed to reason with themselves using languages that they create on their own and that, basically… only *they* can understand.

This is an interesting idea for a few reasons—most important, to me at least, is what the LLM may “hide” from a human examiner. What is encoded in the LLM’s special, un-human-readable language? Further, how does this affect the LLM’s alignment with human needs?

In other words: if we don’t know how it arrives at a certain conclusion or plan of action (via its own special language), how can we, as human evaluators and users, even know or trust that our better nature is being reflected by the AI?

It would seem that, should we employ this method and allow LLMs to construct their own AI inner-language, we will either be totally mathematically prevented from understanding it ourselves and give up, or… we will have to find other, creative ways keep the AI aligned.

A potential heuristic toward developing a solution to this soon-to-be-very-real problem: quantum computing.

(Okay, yes, I’m always looking for QC use-cases, because I love QC and because there basically are no QC use-cases… but let’s go down the weird rabbit-hole and pretend that we may know what we’re talking about.)

The potential of quantum computing being brought to bear on LLMs is tantalizing… And I’m just spit-balling at 5am after a sleepless night worrying myself to death over emergent language… hear me out…

I’m not going to suggest that QC can “translate the alien language of LLMs” (crazy woo-woo territory), but I am going to posit, as people already have, that quantum computing can be used to determine, in an LLM or other AI, which internal representations causally constrain model behavior.

Wow, okay, maybe that reads like a doozy… So what am I saying?

It may be possible, with quantum algorithms, to tease out the structures—the patterns, the logic circuits—in classical LLMs that affect how they behave. We wouldn’t be able to read the language they speak to themselves or “crack the alignment code,” but we would possibly be able to understand the mechanics of a model—why it reacts in a certain way to a given prompt or stimulus.

That could indeed be useful, especially where classical techniques fail. Techniques that are inspired by quantum mechanics, at least, could in theory add more tools to our alignment toolbox.

How much did you hate this article? Or did you get this far? (I wouldn't blame you if you didn't!) Feel free to yell at me on X @Elroy_Muscato :)`
    },
    {
        id: 4,
        title: 'Vibe Coding My Way Into 2026',
        date: 'Dec 23, 2025',
        content: `_Ah, computers! Such intelligent creatures…_

Going into the new year, and I mean really _going into it_ with that strange January cocktail of ambition, mild panic, and an overactive sense of possibility, I realized I wanted to revamp my personal portfolio website for 2026. Not refresh it. Not “iterate” on it. Revamp it in the biblical sense. Burn it down conceptually and rebuild something that actually sounded like me when it spoke.

The old site wasn’t wrong, exactly. It just wasn’t telling the whole truth anymore. It functioned. It was legible. It behaved. But it felt like it was introducing me the way a LinkedIn headline introduces a human being: technically accurate, emotionally vacant. And once I noticed that disconnect, I couldn’t unsee it. A personal site that doesn’t reflect how you think is worse than no site at all. It’s a polite lie.

So I rebuilt it. Conceptually first, obsessively second, and yes—_vibe coded_ the implementation. The result is zachbohl.com. Everything on it is my brainchild. The ideas, the tone, the aesthetic posture, the pacing, the deliberate friction, the moments of playfulness that threaten to undermine seriousness but never quite do. That’s all me. The code is the instrument. I’m the composer.  

And before we go any further, I should also say this plainly, because honesty is sort of the throughline here: **I used ChatGPT to help write this blog post too.** That wasn’t an accident, and it wasn’t laziness. It was consistency. This entire project is about authorship as direction, not martyrdom through manual execution. I know what I want to say. I know how I want it to sound. ChatGPT helps me tune the signal. I still decide when it’s right.

That same philosophy applied to the site itself. Vibe coding, as I practice it, is not abdication of thought. It’s the opposite. It requires you to be _more_ opinionated, not less. When the friction of implementation drops, the only thing left to judge is the quality of the idea. You can’t hide behind effort anymore. Either the thing has a soul, or it doesn’t.

Before anything was built, I knew how I wanted the site to feel. That part took hours. Actual hours of concentrated calibration. Not coding hours—thinking hours. Sitting there adjusting tone in my head. Deciding how confident is too confident, how playful is too playful, how much technicality signals competence without tipping into performative cleverness. That’s the work people don’t see, because it doesn’t leave fingerprints. But it’s the part that matters most.

Aesthetically, I landed on retro-futurism, but not the lazy, neon-synthwave caricature of it. I wanted something closer to optimism-with-edges. A future imagined by people who still believed computers might help us become more interesting, not just more optimized. Chunky interfaces. Bold typography. Motion that feels intentional instead of ornamental. A site that looks like it has opinions and isn’t afraid to express them.

I’ll say this openly: **I drew inspiration from poolsuite.net.** Not in a copy-paste sense, but in the way that good art reminds you what’s possible. Poolsuite understands something fundamental about software and joy, about interfaces that feel alive instead of purely transactional. That gave me permission to lean into personality instead of sanding it down.

Structurally, the site is not optimized for skimming. That’s deliberate. It’s paced. It asks you to linger. I wanted the experience of moving through it to mirror how I approach problems: slowly enough to notice details, fast enough not to bore myself. There are no purely decorative choices. Even the moments of friction are part of the conversation.  

One of my favorite features—and I say this with zero irony—is the **sun and moon cycle that changes based on your system time**. That little detail gives me an unreasonable amount of joy. It’s subtle, but it makes the site feel aware of the world it lives in. Morning feels different from night. Interfaces should acknowledge time. Computers are temporal creatures, after all.

Another feature I love, even though it’s still very much a work in progress, is the **community paint area**. The idea that people can come together and collectively use something like MS Paint on my website scratches a very specific itch in my brain. It’s playful, a little chaotic, and deeply internet-native. It’s not done yet, but that’s okay. I like that it’s becoming rather than finished. So am I.

The music on the site is original, and that’s important to me. None of it is AI-generated. I care a lot about creative authorship, even when I’m happy to use tools to assist execution. Music, especially, still feels like a place where human intention should remain unmistakable. The site sounds like me because it is me, in that sense too.

All of this, incidentally, is happening while I am fully aware that I “should” be studying cybersecurity certifications. That thought hovers over most of my creative work like a benevolent but judgmental ghost. But I’ve learned not to fight that tension. The urge to build expressive systems and the discipline required to secure them are not opposites. They’re part of the same mind. One feeds the other, whether I like it or not.  

There’s been a lot of anxious discourse lately about authorship in the age of AI, about whether something “counts” if you didn’t personally grind through every mechanical step. I think that anxiety misses the point. Authorship has always been about intent, taste, and responsibility. This site reflects my intent. It reflects my taste. It reflects my willingness to say, publicly, this is how I think right now.

The theme of this entire project was captured accidentally in something I once posted on X while waiting for Antigravity to do my bidding:  

> **“Ah, computers! Such intelligent creatures…”**

I meant it half-jokingly at the time, but it’s grown into something more sincere. Computers are strange collaborators. They amplify us. They expose us. They reflect our thinking back at us, sometimes uncomfortably clearly.

This website is a conversation with those creatures. A record of how I’m thinking as we head into 2026. It’s not final. It’s not precious. But after hours of calibration, iteration, rejection, and refinement, I can say this with confidence: **I’m really pleased with the outcome.**

The plumbing is modern. The tools are powerful. The thinking is mine. And for now, at least, it feels honest.

--Zach Bohl (via ChatGPT), Tuesday, December 23, 2025`
    },
    {
        id: 1,
        title: 'Hello, Quantum World — My First Steps with IBM Quantum & Qiskit',
        date: 'Oct 14, 2025',
        content: `## Introduction

I recently followed IBM Quantum's "Hello world" tutorial and got my hands dirty with qubits, entanglement, and the real challenges of running circuits on quantum hardware. In this post I'll walk through what I learned, show code snippets, and reflect on what surprised me (and what I'm excited to try next).

## Setting the Stage

To get started, I set up a Python environment (Jupyter) with Qiskit, qiskit‑ibm-runtime, and matplotlib. I also configured my IBM Quantum credentials so that I could submit jobs to real quantum processors via IBM Cloud.

This setup might seem boilerplate, but it's crucial: quantum frameworks depend heavily on proper versions, backend connectivity, and visualization tools.

## The Four Phases of a Quantum Program

One of the most useful mental models I picked up from the tutorial is that any quantum program (in this Qiskit + IBM runtime setting) can be thought of in four phases:

* Map — translate your problem into circuits and operators
* Optimize — adapt circuits to hardware constraints, reduce depth, map layouts
* Execute — send the job to a simulator or QPU using primitives like Estimator or Sampler
* Analyze — interpret results, plot, use error mitigation

This "pipeline" abstraction is helpful: any nontrivial quantum algorithm you write later will go through these phases.

## A Toy Example: Bell State + Observables

To test things out, I constructed this simple circuit:

\`\`\`python
from qiskit import QuantumCircuit
qc = QuantumCircuit(2)
qc.h(0)
qc.cx(0, 1)
qc.draw("mpl")
\`\`\`

This prepares a Bell entangled state between qubits 0 and 1.

Next, I defined a few observables (Pauli operators) using \`SparsePauliOp\`: \`IZ\`, \`IX\`, \`ZI\`, \`XI\`, \`ZZ\`, \`XX\`.

These capture single‑qubit measurements and correlations. The expectation values of these observables will tell me whether the qubits behave independently or are correlated (entangled).

For execution, I used:

\`\`\`python
from qiskit_ibm_runtime import EstimatorV2 as Estimator
estimator = Estimator(mode=backend)
estimator.options.resilience_level = 1
estimator.options.default_shots = 5000
job = estimator.run([(isa_circuit, mapped_observables)])
\`\`\`

## Scaling Up: GHZ States and the Noise Problem

After verifying things on 2 qubits, the tutorial scales to 100 qubits by preparing a GHZ state:

\`\`\`python
def get_qc_for_n_qubit_GHZ_state(n):
    qc = QuantumCircuit(n)
    qc.h(0)
    for i in range(n - 1):
        qc.cx(i, i+1)
    return qc
\`\`\`

## What Surprised Me & Lessons Learned

* The sheer difference between ideal outcomes and real hardware outputs is humbling.
* The optimization / mapping stage is not cosmetic — it's essential.
* Scaling is brutal. The decay of correlations is a real-world symptom of quantum fragility.

## What's Next For Me

* Dive deeper into error mitigation techniques.
* Try VQE (Variational Quantum Eigensolver) or QAOA.
* Experiment with hybrid quantum-classical workflows.`
    },
    {
        id: 2,
        title: 'Revisiting The Brothers Karamazov at 30',
        date: 'Jul 23, 2025',
        content: `Ten years ago, at the age of 20, I cracked open one of the finest works of literature humanity has ever produced. Did I understand it? Will I understand it now?

*The Brothers Karamazov* by Fyodor Dostoevsky is a beast. 900 pages of Russian philosophy wrestling with faith, evil, family, and murder. I remember sitting in a diner at 1 AM reading Ivan's conversation with the devil. The dread was palpable.

Did I get it then? I'm not sure. I've always "struggled with faith." I've dabbled in other traditions like Hinduism, but it never quite clicked. 

This isn't about fishing for faith; it's a retrospective. I don't remember much of the plot, but I remember the feeling. Back in my late teens, I was a Dostoevsky fan before it was a cliché. Now at 30, after a bumpy ten years, I'm wondering what else the book has to say.

Life threw some wild stuff at me—heartbreaking and hilarious. A retrospective at 30 might seem gauche, but I just want to make cool shit for my bros. 

So, what are you reading? Me? I'm reading Dostoevsky. Again.`
    },
    {
        id: 3,
        title: 'From Quantum Math to Synth Knobs: A Strange Journey Through Brains, Qubits, and Sound',
        date: 'Jan 20, 2025',
        content: `It all started with a simple question: how do you make giant language models faster?

Engineers have found clever ways to trim them down. Quantization shrinks brains into 8-bit or 4-bit. Speculative decoding lets a small "draft model" write ahead. It's all about shaving milliseconds off billions of calculations.

## The Quantum Detour

Qubits live in superpositions, balancing yes and no at the same time. The HHL algorithm promises to solve linear systems exponentially faster, which is at the heart of neural nets. But preparing data is slow, and hardware is noisy. For now, it's a glowing lighthouse on the horizon.

## Kernels, Overlaps, and Attention

In classical ML, kernels are shortcuts for measuring similarity. Quantum systems can encode information as states and perform an overlap test:

\`\`\`
k(x, y) = | <phi(x) | phi(y)> |²
\`\`\`

This looks a lot like what transformers already do with attention. The gap between transformers and quantum algorithms may be smaller than it seems.

## Where Music Sneaks In

Every instrument has an acoustic fingerprint. Could we replace serial numbers with sound tests? Synthesizers like Serum are universes of wavetables and knobs. In theory, you could work backwards from a track to the preset using phase-sensitive overlaps.

## What Ties It All Together

The theme is fingerprints:

* Matrices have eigenvalue fingerprints.
* Attention layers compute similarity fingerprints.
* Instruments carry timbral fingerprints.
* Synth patches hide behind parameter fingerprints.

Whether it's GPUs or qubits, the puzzle is always about identifying the hidden signature in a sea of noise.`
    }
];
